{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Vang In-class-exercise-05.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamel19/INFO5731_FALL2020/blob/master/Vang_In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ApuKiKszES"
      },
      "source": [
        "## The fifth In-class-exercise (9/30/2020, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sioa9-LAszEU"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMr36wBGszEV",
        "outputId": "6b1d31ef-15f9-4615-a4ce-22812fdeb654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install GetOldTweets3\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GetOldTweets3 in /usr/local/lib/python3.6/dist-packages (0.0.11)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Requirement already satisfied: pyquery>=1.2.10 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (1.4.1)\n",
            "Requirement already satisfied: cssselect>0.7.9 in /usr/local/lib/python3.6/dist-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ua0qeYJjyi9"
      },
      "source": [
        "import pandas as pd\n",
        "import GetOldTweets3 as got\n",
        "\n",
        "def get_twitter_info():\n",
        "    tweet_df[\"tweet_text\"] = tweet_df[\"got_criteria\"].apply(lambda x: x.text)\n",
        "    tweet_df[\"date\"] = tweet_df[\"got_criteria\"].apply(lambda x: x.date)\n",
        "    tweet_df[\"hashtags\"] = tweet_df[\"got_criteria\"].apply(lambda x: x.hashtags)\n",
        "    tweet_df[\"link\"] = tweet_df[\"got_criteria\"].apply(lambda x: x.permalink)\n",
        "\n",
        "\n",
        "keyword = \"#covid19\"                        #keyword \n",
        "oldest_date = \"2019-09-12\"                  #oldest date for extraction\n",
        "newest_date = \"2020-09-17\"                  #newest date for extraction\n",
        "locations =[\"New York City\", \"Chicago\", \"Dallas\"]     #location \n",
        "\n",
        "number_tweets =500\n",
        "\n",
        "#get old tweets\n",
        "tweetCriteria_list = []\n",
        "for location in locations:    \n",
        "    try:\n",
        "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(keyword).setSince(oldest_date).setUntil(newest_date).setNear(location).setMaxTweets(number_tweets)\n",
        "    except:\n",
        "        continue   \n",
        "#create twitter info for each city\n",
        "tweet_dict = {}                   \n",
        "for criteria, location in zip(tweetCriteria_list, locations):\n",
        "    tweets = got.manager.TweetManager.getTweets(criteria)\n",
        "    tweet_dict[location] = tweets\n",
        "    \n",
        "#create df\n",
        "tweet_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in tweet_dict.items() ]))\n",
        "tweet_df['tweet_count'] = tweet_df.index\n",
        "tweet_df = pd.melt(tweet_df, id_vars=[\"tweet_count\"], var_name='City', value_name='got_criteria')\n",
        "tweet_df = tweet_df.dropna()\n",
        "\n",
        "#extract twitter information\n",
        "get_twitter_info()\n",
        "tweet_df = tweet_df.drop(\"got_criteria\", 1)\n",
        "tweet_df.head()\n",
        "\n",
        "#export the frame to a csv file\n",
        "tweet_df.to_csv(\"US_results.csv\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP9uQySfj455",
        "outputId": "d37363c1-2afb-4b28-828a-6922f6d03bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "tweet_df"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_count</th>\n",
              "      <th>City</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>date</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [tweet_count, City, tweet_text, date, hashtags, link]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRuD9Y9b3MRl"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "dataset = [\n",
        "    \"Dreams are one of the ways our mind processes emotions, especially intense emotions, so it’s natural to have nightmares when we are under stress. - Dr. Jennifer Martin, on the link between #COVID19 and our dreams\",\n",
        "    \"Sleep more. That's one way to reduce your #COVID19 risk. It also will increase the effectiveness of an eventual vaccine.\",\n",
        "    \"#BREAKING - #COVID19 has claimed at least six more lives in Central Texas' two most populous counties.\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pKBcaoW3PtC",
        "outputId": "58964025-6ace-424f-b515-1fe3e8ebac49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "tfIdf = tfIdfVectorizer.fit_transform(dataset)\n",
        "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "df = df.sort_values('TF-IDF', ascending=False)\n",
        "print (df.head(25))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-59ad278e8daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfIdfVectorizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfIdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfIdfVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfIdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfIdfVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TF-IDF\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF-IDF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8cHHOtVQNYW",
        "outputId": "77c85daf-a310-4159-a66c-6aac2ea4f78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "#load up our sample sentences\n",
        "first= 'Dreams are one of the ways our mind processes emotions, especially intense emotions, so it’s natural to have nightmares when we are under stress. - Dr. Jennifer Martin, on the link between #COVID19 and our dreams'\n",
        "second= '#BREAKING - #COVID19 has claimed at least six more lives in Central Texas two most populous counties.'\n",
        "#split so each word have their own string\n",
        "first = first.split(\" \")\n",
        "second= second.split(\" \")\n",
        "#join them to remove common duplicate words\n",
        "total= set(first).union(set(second))\n",
        "\n",
        "#Now lets add a way to count the words using a dictionary key-value pairing for both sentences\n",
        "wordDictA = dict.fromkeys(total, 0) \n",
        "wordDictB = dict.fromkeys(total, 0)\n",
        "for word in first:\n",
        "    wordDictA[word]+=1\n",
        "    \n",
        "for word in second:\n",
        "    wordDictB[word]+=1\n",
        "#put them in a dataframe and then view the result:\n",
        "pd.DataFrame([wordDictA, wordDictB])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dreams</th>\n",
              "      <th>of</th>\n",
              "      <th>between</th>\n",
              "      <th>mind</th>\n",
              "      <th>under</th>\n",
              "      <th>dreams</th>\n",
              "      <th>our</th>\n",
              "      <th>one</th>\n",
              "      <th>most</th>\n",
              "      <th>six</th>\n",
              "      <th>#BREAKING</th>\n",
              "      <th>we</th>\n",
              "      <th>ways</th>\n",
              "      <th>Martin,</th>\n",
              "      <th>populous</th>\n",
              "      <th>link</th>\n",
              "      <th>#COVID19</th>\n",
              "      <th>emotions,</th>\n",
              "      <th>and</th>\n",
              "      <th>intense</th>\n",
              "      <th>it’s</th>\n",
              "      <th>when</th>\n",
              "      <th>so</th>\n",
              "      <th>has</th>\n",
              "      <th>at</th>\n",
              "      <th>claimed</th>\n",
              "      <th>Central</th>\n",
              "      <th>to</th>\n",
              "      <th>in</th>\n",
              "      <th>-</th>\n",
              "      <th>lives</th>\n",
              "      <th>nightmares</th>\n",
              "      <th>are</th>\n",
              "      <th>processes</th>\n",
              "      <th>Dr.</th>\n",
              "      <th>stress.</th>\n",
              "      <th>Jennifer</th>\n",
              "      <th>natural</th>\n",
              "      <th>on</th>\n",
              "      <th>least</th>\n",
              "      <th>counties.</th>\n",
              "      <th>Texas</th>\n",
              "      <th>have</th>\n",
              "      <th>more</th>\n",
              "      <th>the</th>\n",
              "      <th>two</th>\n",
              "      <th>especially</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dreams  of  between  mind  under  ...  have  more  the  two  especially\n",
              "0       1   1        1     1      1  ...     1     0    2    0           1\n",
              "1       0   0        0     0      0  ...     0     1    0    1           0\n",
              "\n",
              "[2 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7X5lOkC4dZu",
        "outputId": "ed26860c-280c-4dfb-e427-003165c64592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import spacy \n",
        "  \n",
        "# Load English tokenizer, tagger,  \n",
        "# parser, NER and word vectors \n",
        "nlp = spacy.load(\"en_core_web_sm\") \n",
        "  \n",
        "# Process whole documents \n",
        "text = (\"\"\"#Parent: Keep your #child healthy by bringing them for their routine well-child and #vaccine visits. \n",
        "These visits continue to be important during the #COVID19 pandemic. Learn more: https://bit.ly/2QYkOcx..\"\"\") \n",
        "  \n",
        "doc = nlp(text) \n",
        "  \n",
        "# Token and Tag \n",
        "for token in doc: \n",
        "  print(token, token.pos_) \n",
        "  \n",
        "# You want list of Verb tokens \n",
        "print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"]) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# PROPN\n",
            "Parent NOUN\n",
            ": PUNCT\n",
            "Keep VERB\n",
            "your DET\n",
            "# NOUN\n",
            "child NOUN\n",
            "healthy ADJ\n",
            "by ADP\n",
            "bringing VERB\n",
            "them PRON\n",
            "for ADP\n",
            "their DET\n",
            "routine ADJ\n",
            "well NOUN\n",
            "- PUNCT\n",
            "child NOUN\n",
            "and CCONJ\n",
            "# NOUN\n",
            "vaccine NOUN\n",
            "visits NOUN\n",
            ". PUNCT\n",
            "\n",
            " SPACE\n",
            "These DET\n",
            "visits NOUN\n",
            "continue VERB\n",
            "to PART\n",
            "be AUX\n",
            "important ADJ\n",
            "during ADP\n",
            "the DET\n",
            "# SYM\n",
            "COVID19 ADJ\n",
            "pandemic NOUN\n",
            ". PUNCT\n",
            "Learn VERB\n",
            "more ADV\n",
            ": PUNCT\n",
            "https://bit.ly/2QYkOcx PROPN\n",
            ".. PUNCT\n",
            "Verbs: ['Keep', 'bringing', 'continue', 'Learn']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6tRHs1T44a9",
        "outputId": "2ef916c7-6753-4ea5-c7d8-388ed2f54f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"As of September 27, #COVID19 cases were trending upward in 26 states, especially in the West and central parts of the country. In the last week, 6 states reported more than 10,000 new cases. Wear a mask. Wash your hands. Stay 6 feet from others. More: https://bit.ly/3n2jyUd\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "As as SCONJ IN prep Xx True True\n",
            "of of ADP IN prep xx True True\n",
            "September September PROPN NNP pobj Xxxxx True False\n",
            "27 27 NUM CD nummod dd False False\n",
            ", , PUNCT , punct , False False\n",
            "# # SYM $ dep # False False\n",
            "COVID19 covid19 NOUN NN compound XXXXdd False False\n",
            "cases case NOUN NNS nsubj xxxx True False\n",
            "were be AUX VBD aux xxxx True True\n",
            "trending trend VERB VBG ROOT xxxx True False\n",
            "upward upward ADV RB advmod xxxx True False\n",
            "in in ADP IN prep xx True True\n",
            "26 26 NUM CD nummod dd False False\n",
            "states state NOUN NNS pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "especially especially ADV RB advmod xxxx True False\n",
            "in in ADP IN prep xx True True\n",
            "the the DET DT det xxx True True\n",
            "West West PROPN NNP pobj Xxxx True False\n",
            "and and CCONJ CC cc xxx True True\n",
            "central central ADJ JJ amod xxxx True False\n",
            "parts part NOUN NNS conj xxxx True False\n",
            "of of ADP IN prep xx True True\n",
            "the the DET DT det xxx True True\n",
            "country country NOUN NN pobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "In in ADP IN prep Xx True True\n",
            "the the DET DT det xxx True True\n",
            "last last ADJ JJ amod xxxx True True\n",
            "week week NOUN NN pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "6 6 NUM CD nummod d False False\n",
            "states state NOUN NNS nsubj xxxx True False\n",
            "reported report VERB VBD ROOT xxxx True False\n",
            "more more ADJ JJR amod xxxx True True\n",
            "than than SCONJ IN quantmod xxxx True True\n",
            "10,000 10,000 NUM CD nummod dd,ddd False False\n",
            "new new ADJ JJ amod xxx True False\n",
            "cases case NOUN NNS dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Wear wear VERB VB ROOT Xxxx True False\n",
            "a a DET DT det x True True\n",
            "mask mask NOUN NN dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Wash wash VERB VB ROOT Xxxx True False\n",
            "your -PRON- DET PRP$ poss xxxx True True\n",
            "hands hand NOUN NNS dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n",
            "Stay stay VERB VB ROOT Xxxx True False\n",
            "6 6 NUM CD nummod d False False\n",
            "feet foot NOUN NNS npadvmod xxxx True False\n",
            "from from ADP IN prep xxxx True True\n",
            "others other NOUN NNS pobj xxxx True True\n",
            ". . PUNCT . punct . False False\n",
            "More More ADJ JJR ROOT Xxxx True True\n",
            ": : PUNCT : punct : False False\n",
            "https://bit.ly/3n2jyUd https://bit.ly/3n2jyud X FW punct xxxx://xxx.xx/dxdxxXx False False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}